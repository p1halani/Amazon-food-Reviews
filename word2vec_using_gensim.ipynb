{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "# Ignore  the warnings\n",
    "import warnings \n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore') \n",
    "# data visualisation and manipulation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns \n",
    "# sets matplotlib to inline and displays graphs belo w the corressponding cell. \n",
    "% matplotlib inline\n",
    "style.use('fivethirtyeight') \n",
    "sns.set(style='whitegrid',color_codes=True) \n",
    "#nltk \n",
    "import nltk \n",
    "#preprocessing \n",
    "from nltk.corpus import stopwords  #stopwords \n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing \n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others from nltk.stem.snowball import SnowballStemmer from nltk.stem import WordNetLemmatizer  # lammatiz er from WordNet \n",
    "# for part-of-speech tagging\n",
    "from nltk import pos_tag \n",
    "# for named entity recognition (NER) \n",
    "from nltk import ne_chunk \n",
    "# vectorizers for creating the document-term-matrix (DTM) \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer \n",
    "# BeautifulSoup libraray \n",
    "from bs4 import BeautifulSoup  \n",
    "import re # regex \n",
    "#model_selection \n",
    "from sklearn.model_selection import train_test_split,cross_validate \n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "#evaluation \n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.metrics import classification_report \n",
    "from mlxtend.plotting import plot_confusion_matrix \n",
    "#preprocessing scikit\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder \n",
    "#classifiaction. \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import LinearSVC,SVC \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB \n",
    "#stop-words \n",
    "stop_words=set(nltk.corpus.stopwords.words('english')) \n",
    "#keras\n",
    "import keras \n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Embedding,Input,CuDNNLSTM,LSTM \n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "#gensim word2vec \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING THE DATASET\n",
    "reviews_fram=pd.read_csv(\"../amazon-fine-food-reviews/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=reviews_fram.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['Text','Score']] \n",
    "df['review']=df['Text'] \n",
    "df['rating']=df['Score']\n",
    "df.drop(['Text','Score'],axis=1,inplace=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "print(df['rating'].isnull().sum())\n",
    "df['review'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates for every duplicate we will keep only one row of that type. \n",
    "df.drop_duplicates(subset=['rating','review'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check the shape.  \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First break text into sentences and then clean those sentences.\n",
    "#since we are doing sentiment analysis, convert the values in score column to sentiment. Sentiment is 0 for ratings or scores less than 3 and 1 or more elsewhere.\n",
    "def mark_sentiment(rating):  \n",
    "    if(rating<=3):    \n",
    "        return 0  \n",
    "    else:   \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment']=df['rating'].apply(mark_sentiment)\n",
    "df.drop(['rating'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation and html tags if any.\n",
    "# Remove the stop words and shorter words as they cause noise.\n",
    "# Stem or Lemmatize the words depending on what does better.\n",
    "\n",
    "# function to clean and pre-process the text. \n",
    "def clean_reviews(review):          \n",
    "    # 1. Removing html tags    \n",
    "    review_text = BeautifulSoup(review,\"lxml\").get_text()        \n",
    "    # 2. Retaining only alphabets.  \n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting    \n",
    "    word_tokens= review_text.lower().split() \n",
    "    # 4. Remove stopwords    \n",
    "    le=WordNetLemmatizer()    \n",
    "    stop_words= set(stopwords.words(\"english\"))    \n",
    "    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]  \n",
    "    \n",
    "    cleaned_review=\" \".join(word_tokens)   \n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To balance the class, taken equal instances of each sentiment.\n",
    "pos_df=df.loc[df.sentiment==1,:][:50000] \n",
    "neg_df=df.loc[df.sentiment==0,:][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining \n",
    "df=pd.concat([pos_df,neg_df],ignore_index=True) \n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling rows \n",
    "df = df.sample(frac=1).reset_index(drop=True) \n",
    "print(df.shape) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences=[]\n",
    "sum=0\n",
    "for review in df['review']:\n",
    "    sents=tokenizer.tokenize(review.strip())\n",
    "    sum+=len(sents)\n",
    "    for sent in sents:\n",
    "        cleaned_sent=clean_reviews(sent)\n",
    "        sentences.append(cleaned_sent.split()) # can use word_tokenize also.\n",
    "print(sum)\n",
    "print(len(sentences))  # total no of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to print few sentences\n",
    "for te in sentences[:5]:\n",
    "    print(te,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model=gensim.models.Word2Vec(sentences=sentences,size=300,window=10,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(sentences,epochs=10,total_examples=len(sentences))# embedding of a particular word.\n",
    "w2v_model.wv.get_vector('fair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total numberof extracted words.\n",
    "vocab=w2v_model.wv.vocab\n",
    "print(\"The total number of words are : \",len(vocab))\n",
    "# words most similar to a given word.\n",
    "w2v_model.wv.most_similar('fair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similaraity b/w two words\n",
    "w2v_model.wv.similarity('fair','reasonable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The no of words :\",len(vocab))\n",
    "# print(vocab)\n",
    "vocab=list(vocab.keys())\n",
    "word_vec_dict={}\n",
    "for word in vocab:\n",
    "    word_vec_dict[word]=w2v_model.wv.get_vector(word)\n",
    "print(\"The no of key-value pairs : \",len(word_vec_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning reviews.\n",
    "df['clean_review']=df['review'].apply(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words = 56379.\n",
    "\n",
    "# now since we will have to pad we need to find the maximum lenght of any document.\n",
    "\n",
    "maxi=-1\n",
    "for i,rev in enumerate(df['clean_review']):\n",
    "    tokens=rev.split()\n",
    "    if(len(tokens)>maxi):\n",
    "        maxi=len(tokens)\n",
    "print(maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(df['clean_review'])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "encd_rev = tok.texts_to_sequences(df['clean_review'])\n",
    "max_rev_len=1565  # max lenght of a review\n",
    "vocab_size = len(tok.word_index) + 1  # total no of words\n",
    "embed_dim=300 # embedding dimension as choosen in word2vec constructor\n",
    "# now padding to have a amximum length of 1565\n",
    "pad_rev= pad_sequences(encd_rev, maxlen=max_rev_len, padding='post')\n",
    "pad_rev.shape   # note that we had 100K reviews and we have padded each review to have  a lenght of 1565 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now creating the embedding matrix\n",
    "embed_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
    "for word,i in tok.word_index.items():\n",
    "    embed_vector=word_vec_dict.get(word)\n",
    "    if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "    \n",
    "     embed_matrix[i]=embed_vector\n",
    "  # if word is not found then embed_vector corressponding to that vector will stay zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_matrix[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train and val sets first\n",
    "Y=keras.utils.to_categorical(df['sentiment'])  # one hot target as required by NN.\n",
    "x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Dropout\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_rev_len,embeddings_initializer=Constant(embed_matrix)))\n",
    "# model.add(CuDNNLSTM(64,return_sequences=False)) # loss stucks at about \n",
    "model.add(Flatten())\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "# model.add(Dense(16,activation='relu'))\n",
    "# model.add(Dropout(0.20))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size=64\n",
    "# fitting the model.\n",
    "model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}