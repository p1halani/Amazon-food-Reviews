{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##################Data Loading Step##################\\n')\n",
    "\n",
    "conn = sqlite3.connect('amazon-fine-food-reviews/database.sqlite')\n",
    "filtered_data = pd.read_sql_query(''' SELECT * FROM REVIEWS LIMIT 100000''', conn)\n",
    "\n",
    "# Give reviews with Score>3 a positive rating(1), and reviews with a score<3 a negative rating(0).\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def findMinorClassPoints(df):\n",
    "    posCount = int(df[df['Score']==1].shape[0]);\n",
    "    negCount = int(df[df['Score']==0].shape[0]);\n",
    "    if negCount < posCount:\n",
    "        return negCount\n",
    "    return posCount\n",
    "\n",
    "#changing reviews with score less than 3 to be positive and vice-versa\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition)\n",
    "filtered_data['Score'] = positiveNegative\n",
    "\n",
    "#Performing Downsampling\n",
    "# samplingCount = findMinorClassPoints(filtered_data)\n",
    "# postive_df = filtered_data[filtered_data['Score'] == 1].sample(n=5000)\n",
    "# negative_df = filtered_data[filtered_data['Score'] == 0].sample(n=5000)\n",
    "\n",
    "# filtered_data = pd.concat([postive_df, negative_df])\n",
    "\n",
    "print(\"Number of data points in our data\", filtered_data.shape)\n",
    "# filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##################Preprocessing Step##################\\n')\n",
    "\n",
    "#Sorting data according to ProductId in ascending order\n",
    "sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "#Deduplication of entries\n",
    "final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "final.shape\n",
    "\n",
    "#Removing the anamolies\n",
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n",
    "\n",
    "#Preprocessing\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "preprocessed_reviews = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(final['Text'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    # sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_reviews.append(sentance.strip())\n",
    "    \n",
    "## Similartly you can do preprocessing for review summary also.\n",
    "def concatenateSummaryWithText(str1, str2):\n",
    "    return str1 + ' ' + str2\n",
    "\n",
    "preprocessed_summary = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentence in tqdm(final['Summary'].values):\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    #sentence = BeautifulSoup(sentence, 'lxml').get_text()\n",
    "    sentence = decontracted(sentence)\n",
    "    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n",
    "    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    # sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n",
    "    preprocessed_summary.append(sentence.strip())\n",
    "    \n",
    "preprocessed_reviews = list(map(concatenateSummaryWithText, preprocessed_reviews, preprocessed_summary))\n",
    "final['CleanedText'] = preprocessed_reviews\n",
    "final['CleanedText'] = final['CleanedText'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final['CleanedText']\n",
    "y = final['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final\n",
    "del preprocessed_reviews\n",
    "del preprocessed_summary\n",
    "del sorted_data\n",
    "del filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##################Shapes of train, val, test data##################\\n')\n",
    "\n",
    "X_t, X_test, y_t, y_test = train_test_split(X, y, test_size=0.20, stratify=y, shuffle=True)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_t, y_t, test_size=0.20, stratify=y_t, shuffle=True)\n",
    "print(\"Shape of Input  - Train:\", X_train.shape)\n",
    "print(\"Shape of Output - Train:\", y_train.shape)\n",
    "print(\"Shape of Input  - CV   :\", X_cv.shape)\n",
    "print(\"Shape of Output - CV   :\", y_cv.shape)\n",
    "print(\"Shape of Input  - Test :\", X_test.shape)\n",
    "print(\"Shape of Output - Test :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = Tokenizer(num_words=5000)\n",
    "tokenize.fit_on_texts(X_train)\n",
    "\n",
    "X_train_new = tokenize.texts_to_sequences(X_train)\n",
    "X_cv_new = tokenize.texts_to_sequences(X_cv)\n",
    "X_test_new = tokenize.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and/or pad input sequences\n",
    "max_review_length = 1000\n",
    "X_train_new = sequence.pad_sequences(X_train_new, maxlen=max_review_length)\n",
    "X_cv_new = sequence.pad_sequences(X_cv_new, maxlen=max_review_length)\n",
    "X_test_new = sequence.pad_sequences(X_test_new, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
    "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
    "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "n_epochs = 5\n",
    "batchsize = 512\n",
    "\n",
    "final_output = pd.DataFrame(columns=[\"Model\", \"Architecture\",\n",
    "                                     \"TRAIN_LOSS\", \"TEST_LOSS\", \"TRAIN_ACC\", \"TEST_ACC\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model M1 ( Embedding -> LSTM -> Output(Sigmoid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "embed_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, embed_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"***********************************************\")\n",
    "print(\"Printing the Model Summary\")\n",
    "print(model.summary())\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hist = model.fit(X_train_new, y_train, epochs=n_epochs, \n",
    "                   batch_size=batchsize, verbose=1, validation_data=(X_cv_new, y_cv))\n",
    "\n",
    "score = model.evaluate(X_test_new, y_test, batch_size=batchsize)\n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "final_output = final_output.append({\"Model\": 1,\n",
    "                                    \"Architecture\": 'Embedding-LSTM-Sigmoid', \n",
    "                                    \"TRAIN_LOSS\": '{:.5f}'.format(m_hist.history[\"loss\"][n_epochs-1]),\n",
    "                                    \"TEST_LOSS\": '{:.5f}'.format(score[0]),\n",
    "                                    \"TRAIN_ACC\": '{:.5f}'.format(m_hist.history[\"accuracy\"][n_epochs-1]),\n",
    "                                    \"TEST_ACC\": '{:.5f}'.format(score[1])}, ignore_index=True)\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "# list of epoch numbers\n",
    "x = list(range(1,n_epochs+1))\n",
    "\n",
    "vy = m_hist.history['val_loss']\n",
    "ty = m_hist.history['loss']\n",
    "plt_dynamic(x, vy, ty, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model M2 ( Embedding -> LSTM -> Dropout -> Dense(128-Relu) -> Dropout -> Dense (64-Relu) -> Dropout -> Output(Sigmoid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "embed_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, embed_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"***********************************************\")\n",
    "print(\"Printing the Model Summary\")\n",
    "print(model.summary())\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hist = model.fit(X_train_new, y_train, epochs=n_epochs, \n",
    "                   batch_size=batchsize, verbose=1, validation_data=(X_cv_new, y_cv))\n",
    "\n",
    "score = model.evaluate(X_test_new, y_test, batch_size=batchsize)\n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "final_output = final_output.append({\"Model\": 2,\n",
    "                                    \"Architecture\": 'Embedding-LSTM-Dropout-Dense(128-Relu)-Dropout-Dense(64-Relu)-Dropout-Sigmoid', \n",
    "                                    \"TRAIN_LOSS\": '{:.5f}'.format(m_hist.history[\"loss\"][n_epochs-1]),\n",
    "                                    \"TEST_LOSS\": '{:.5f}'.format(score[0]),\n",
    "                                    \"TRAIN_ACC\": '{:.5f}'.format(m_hist.history[\"accuracy\"][n_epochs-1]),\n",
    "                                    \"TEST_ACC\": '{:.5f}'.format(score[1])}, ignore_index=True)\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "# list of epoch numbers\n",
    "x = list(range(1,n_epochs+1))\n",
    "\n",
    "vy = m_hist.history['val_loss']\n",
    "ty = m_hist.history['loss']\n",
    "plt_dynamic(x, vy, ty, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model M3 ( Embedding -> LSTM -> LSTM -> Output(Sigmoid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "embed_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, embed_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"***********************************************\")\n",
    "print(\"Printing the Model Summary\")\n",
    "print(model.summary())\n",
    "print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hist = model.fit(X_train_new, y_train, epochs=n_epochs, \n",
    "                   batch_size=batchsize, verbose=1, validation_data=(X_cv_new, y_cv))\n",
    "\n",
    "score = model.evaluate(X_test_new, y_test, batch_size=batchsize)\n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "final_output = final_output.append({\"Model\": 3,\n",
    "                                    \"Architecture\": 'Embedding-LSTM-LSTM-Sigmoid', \n",
    "                                    \"TRAIN_LOSS\": '{:.5f}'.format(m_hist.history[\"loss\"][n_epochs-1]),\n",
    "                                    \"TEST_LOSS\": '{:.5f}'.format(score[0]),\n",
    "                                    \"TRAIN_ACC\": '{:.5f}'.format(m_hist.history[\"accuracy\"][n_epochs-1]),\n",
    "                                    \"TEST_ACC\": '{:.5f}'.format(score[1])}, ignore_index=True)\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "# list of epoch numbers\n",
    "x = list(range(1,n_epochs+1))\n",
    "\n",
    "vy = m_hist.history['val_loss']\n",
    "ty = m_hist.history['loss']\n",
    "plt_dynamic(x, vy, ty, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}